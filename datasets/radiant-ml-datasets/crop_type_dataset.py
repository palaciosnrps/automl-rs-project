# -*- coding: utf-8 -*-
"""RadiantEarth Starter JW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wr_Z5hw6qEr5jBfQvhonKK4a1qHvaTSu

# Starter Notebook - RadiantEarth Crop ID challenge (ICLR #2)
By Jonathan W.

This takes you through a quick entry into the zindi contest here: https://zindi.africa/competitions/iclr-workshop-challenge-2-radiant-earth-computer-vision-for-crop-recognition


- Download the data (tif files from radiant earth)

# Data Download

This section is essentially a clone of the proviced starter notebooks: https://github.com/radiantearth/mlhub-tutorials/tree/master/notebooks/2020%20CV4A%20Crop%20Type%20Challenge
"""

# Required libraries
import requests
from urllib.parse import urlparse 
from pathlib import Path
from datetime import datetime
#You need sign up for API access in Radiant-mlhub https://dashboard.mlhub.earth/
API =  'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlJqa3dNMEpFTURsRlFrSXdOemxDUlVZelJqQkdPRFpHUVRaRVFqWkRNRVJGUWpjeU5ERTFPQSJ9.eyJpc3MiOiJodHRwczovL3JhZGlhbnRlYXJ0aC5hdXRoMC5jb20vIiwic3ViIjoiYXV0aDB8NWVjOThkODM0NmZiNTkwYzYwMDcyOTQ1IiwiYXVkIjpbImh0dHBzOi8vYXBpLnJhZGlhbnQuZWFydGgvdjEiLCJodHRwczovL3JhZGlhbnRlYXJ0aC5hdXRoMC5jb20vdXNlcmluZm8iXSwiaWF0IjoxNjEwMzYzNzIxLCJleHAiOjE2MTA5Njg1MjEsImF6cCI6IlAzSXFMcWJYUm0xMEJVSk1IWEJVdGU2U0FEbjBTOERlIiwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBlbWFpbCIsInBlcm1pc3Npb25zIjpbXX0.Bh25yfwD0Tof39fn4pHJXRfBnt0v2KEW_U6q_SOSgaLJ7NupMo_a3yzJaAfcXgqIpz3CdHKB2HwBz8iZn6nLBFvPectCYfS8BoCV6oCCnZ6aqidlbXZ8k0FvoTVoMG_YFse8BzqApzpbTLpnd3CaWKmQOOmgrGCDkqnZdS3Da9n18Nge-to_3h564oWD_SuoSYwmbqxqh6-38JBS4XrentUJJV0s8gyCfYYslPj0Od4HkRiFBXPYzKNyXgVGYsXlXfdT1IOG_GlXl4kqFh-sY2SD7jP9891OpVM4FP-UwEkzo5kIKza9rU7kWYKW-sydErO-RBPU91Of526CACzTdQ'
#Define output data path
output_path = Path("/scratch/palacios/data/crop_type")

# these headers will be used in each request
headers = {
    'Authorization': f'Bearer {API}',
    'Accept':'application/json'
}


def get_download_url(item, asset_key, headers):
    asset = item.get('assets', {}).get(asset_key, None)
    if asset is None:
        print(f'Asset "{asset_key}" does not exist in this item')
        return None
    r = requests.get(asset.get('href'), headers=headers, allow_redirects=False)
    return r.headers.get('Location')


def download_file(url):
    filename = urlparse(url).path.split('/')[-1]
    r = requests.get(url)
    f = open(filename, 'wb')
    for chunk in r.iter_content(chunk_size=512 * 1024): 
        if chunk:
            f.write(chunk)
    f.close()
    print(f'Downloaded {filename}')
    return
    
def download_label(url, output_path, tileid):
    filename = urlparse(url).path.split('/')[-1]
    outpath = output_path/tileid
    outpath.mkdir(parents=True, exist_ok=True)
    
    r = requests.get(url)
    f = open(outpath/filename, 'wb')
    for chunk in r.iter_content(chunk_size=512 * 1024): 
        if chunk:
            f.write(chunk)
    f.close()
    print(f'Downloaded {filename}')
    return 

def download_imagery(url, output_path, tileid, date):
    filename = urlparse(url).path.split('/')[-1]
    outpath = output_path/tileid/date
    outpath.mkdir(parents=True, exist_ok=True)
    
    r = requests.get(url)
    f = open(outpath/filename, 'wb')
    for chunk in r.iter_content(chunk_size=512 * 1024): 
        if chunk:
            f.write(chunk)
    f.close()
    print(f'Downloaded {filename}')
    return

# paste the id of the labels collection:
collectionId ='ref_african_crops_kenya_02_labels'

# these optional parameters can be used to control what items are returned. 
#Here, we want to download all the items so:
limit = 10000
bounding_box = []
date_time = []

# retrieves the items and their metadata in the collection
r = requests.get(f'https://api.radiant.earth/mlhub/v1/collections/{collectionId}/items', params={'limit':limit, 'bbox':bounding_box,'datetime':date_time}, headers=headers)
collection = r.json()

# retrieve list of features (in this case tiles) in the collection
for feature in collection.get('features', []):
    assets = feature.get('assets').keys()
    print("Feature", feature.get('id'), 'with the following assets', list(assets))
    #Feature ref_african_crops_kenya_02_tile_03_label with the following assets ['field_ids', 'field_train_test_ids', 'labels']

for feature in collection.get('features', []):
    
    tileid = feature.get('id')#.split('tile_')[-1][:2]
    print(tileid)

    # download labels
    download_url = get_download_url(feature, 'labels', headers)
    download_label(download_url, output_path, tileid)
    
    #download field_ids
#    download_url = get_download_url(feature, 'field_ids', headers)
 #   download_label(download_url, output_path, tileid)

# paste the id of the imagery collection:
collectionId = 'ref_african_crops_kenya_02_source'

# retrieves the items and their metadata in the collection
r = requests.get(f'https://api.radiant.earth/mlhub/v1/collections/{collectionId}/items', params={'limit':limit, 'bbox':bounding_box,'datetime':date_time}, headers=headers)
collection = r.json()

# List assets of the items
for feature in collection.get('features', []):
    assets = feature.get('assets').keys()
    print(list(assets))
    break #all the features have the same type of assets. for simplicity we break the loop here.

# This cell downloads all the multi-spectral images throughout the growing season for this competition.
# The size of data is about 1.5 GB, and download time depends on your internet connection. 
# Note that you only need to run this cell and download the data once.
i = 0
for feature in collection.get('features', []):
    assets = feature.get('assets').keys()
    tileid = feature.get('id').split('tile_')[-1][:2]
    date = datetime.strftime(datetime.strptime(feature.get('properties')['datetime'], "%Y-%m-%dT%H:%M:%SZ"), "%Y%m%d")
    for asset in assets:
        i += 1
        if i > 0: # if resuming after it failed
          download_url = get_download_url(feature, asset, headers)
          download_imagery(download_url, output_path, tileid, date)
